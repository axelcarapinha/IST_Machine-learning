\documentclass{article}
\usepackage{amsmath}  % For math symbols
\usepackage{array}

\title{Machine Learning Report}
\author{}
\date{}
\begin{document}

\maketitle
\begin{center}
    \textbf{Homework 2 - Bayesian Learning}
\end{center}

\section{Bayesian Classifier}
\subsection{Exercise a)}
We will first estimate the priors.

$$p(Class = A) = \frac{4}{8} = \frac{1}{2}$$
$$p(Class = B) = \frac{4}{8} = \frac{1}{2}$$

Now, to estimate the likelihoods, we must consider each distribution as independent (Naive Bayes's assumption), so it will be:
\\
\begin{tabular}{|c|c|c|}
    \hline
    & \( p(x_1 \mid \text{Class} = A) \) & \( p(x_1 \mid \text{Class} = B) \) \\ 
    \hline
    \( \mu \) & 1.25 & 1.2 \\
    \hline
    \( \sigma \) & 0.5508 & 0.6055 \\
    \hline
\end{tabular}
\\
\\
Now, for the query vector:
\begin{align*}
    p(\text{Class} = A \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1 \mid \text{Class} = A) \cdot p(x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(1 \mid \mu = 1.25, \sigma = 0.5508) \cdot N(2 \mid \mu = 1.2, \sigma = 0.6055)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{0.0899}{p(x_1 = 1, x_2 = 2)}
\end{align*}


\begin{align*}
    p(\text{Class} = B \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1 \mid \text{Class} = B) \cdot p(x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(1 \mid \mu = 2.7500\sigma = 0.9574) \cdot N(2 \mid \mu = 0.5500, \sigma = 0.6403)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{0.0019}{p(x_1 = 1, x_2 = 2)}
\end{align*}
\\
By comparing the numerators, we conclude that:
$$p(\text{Class} = A \mid x_1 = 1, x_2 = 2) > p(\text{Class} = B \mid x_1 = 1, x_2 = 2)$$
\\
Hence, the most probable class for the query vector $$x = (1, 2)\textsuperscript{T}$$ is class A.
\newpage



\subsection{Exercise b)}
As before, the priors have the following probabilities:

$$p(Class = A) = \frac{1}{2}$$
$$p(Class = B) = \frac{1}{2}$$

Now, to find the parameters of the two class conditional 2-d Gaussians that model the likelihoods:
\\
\\
\begin{tabular}{|c|c|c|}
    \hline
    & \( p(x_1, x_2 \mid \text{Class} = A) \) & \( p(x_1, x_2 \mid \text{Class} = B) \) \\ 
    \hline
    \( \mu \) & $\begin{pmatrix} 1.25 \\ 1.20 \end{pmatrix}$ & $\begin{pmatrix} 2.75 \\  0.55 \end{pmatrix}$ \\
    \hline
    \( \sum \) & $\begin{pmatrix} 0.3033 & 0.3267 \\ 0.3267 & 0.36667 \end{pmatrix}$ & $\begin{pmatrix} 0.9166 & 0.2500 \\ 0.2500 & 0.4100 \end{pmatrix}$ \\
    \hline
\end{tabular}
\\
\\
Now, calculating the posteriors:
\begin{align*}
    p(\text{Class} = A \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1 \mid \text{Class} = A) \cdot p(x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(
        \begin{pmatrix}1 \\ 2\end{pmatrix} \mid
        \mu = \begin{pmatrix}1.25 \\ 1.20\end{pmatrix},
        \sum = \begin{pmatrix}0.3033 & 0.3267 \\ 0.3267 & 0.36667 \end{pmatrix}
    )}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{4.3346 \times 10\textsuperscript{-17}}{p(x_1 = 1, x_2 = 2)}
\end{align*}

\begin{align*}
    p(\text{Class} = B \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1 \mid \text{Class} = B) \cdot p(x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(
        \begin{pmatrix}1 \\ 2\end{pmatrix} \mid
        \mu = \begin{pmatrix}2.75 \\ 0.55\end{pmatrix},
        \sum = \begin{pmatrix}0.9167 & 0.2500 \\ 0.2500 & 0.4100 \end{pmatrix}
    )}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{2.3373\times10\textsuperscript{-4}}{p(x_1 = 1, x_2 = 2)}
\end{align*}

By comparing the numerators, we conclude that:
$$p(\text{Class} = A \mid x_1 = 1, x_2 = 2) < p(\text{Class} = B \mid x_1 = 1, x_2 = 2)$$
\\
Hence, the most probable class for the query vector $$x = (1, 2)\textsuperscript{T}$$ is class B.
The predicted class is not the same, which may indicate that the parameters \(x_1\) and \(x_2\) are not independent, making a Naive Gaussian distribution inadequate for this situation.
The parameters \(x_1\) and \(x_2\) are likely not independent.
\\
A more complex model that accounts for the joint distribution of the features could provide better predictive accuracy.

\subsection{Exercise c)}
In this exercise, we want to determine which class (A or B) appears the most times when \( x_3 = 1 \).

To compute that, we use the Bayes' rule of interpretation:

\[
P(C \mid E) = \frac{P(E \mid C) \cdot P(C)}{P(E)}
\]

Where:
\begin{itemize}
    \item \( P(C \mid E) \) is the posterior probability of the class \( C \), given the evidence \( E \) (in this case, \( x_3 = 1 \)).
    \item \( P(E \mid C) \) is the likelihood of observing \( x_3 = 1 \) given class \( C \).
    \item \( P(C) \) is the prior probability of class \( C \) (the general probability of class \( C \)).
    \item \( P(E) \) is the total probability of observing \( x_3 = 1 \) (the normalizing factor).
\end{itemize}

Given the following probabilities:
\[
P(A) = \frac{1}{2},\;P(B) = \frac{1}{2},\;P(E \mid A ) = \frac{1}{2},\;P(E \mid B) = \frac{3}{4},\;P(E) = \frac{5}{8}
\]

We can now calculate both final probabilities:

\[P(A \mid E) = \frac{\frac{1}{2} \times \frac{1}{2}}{\frac{5}{8}} = \frac{2}{5}\]

\[P(B \mid E) = \frac{\frac{1}{2} \times \frac{3}{4}}{\frac{5}{8}} = \frac{3}{5}\]

So, with the calculations, we conclude that the most probable class is B.

\subsection{Exercise d) Axel}
We will need to calculate two probabilities, defined as follows:
\begin{align*}
    p(A, x_\text{query}) = p((1,2) | A) \cdot p(1 | A) \cdot p(A) \\
    p(B, x_\text{query}) = p((1,2) | B) \cdot p(1 | B) \cdot p(B) \\
\end{align*}
The higher probability between those will indicate the most probable class for the query vector \( x_\text{query} = (1,2) \). For simplicity, its probability will be referred as \(p((1,2))\), instead of \(p(x_1 = 1, x_2 = 2)\).
Noteworthy, this calculations will be of 3 main parts.

\bigskip
In the \underline{first part}, and given the probabilities:

\[
P(A) = \frac{1}{2},\;P(B) = \frac{1}{2}
\]

We can calculate the probability of \( x_3 \) being 1, with the class being both A and B:

\[
P(x_3 = 1 \mid A) = \frac{card(A.1)}{card(A)} = \frac{1}{2}
\]
\[
P(x_3 = 1 \mid B) = \frac{card(B.1)}{card(B)} = \frac{3}{4}
\]
\bigskip

For the \underline{second part}, and remembering Bayes' theorem, we can state that:
\[
p(A | (1,2)) = \frac{p((1,2) | A) \cdot p(A)}{p((1,2))}
\]

From this, we can rearrange to find the likelihood \( p((1,2) | A) \):
\[
p((1,2) | A) = \frac{p(A | (1,2)) \cdot p((1,2))}{p(A)}
\]

Substituting the known values into the formula:
\[
p((1,2) | A) = \frac{\frac{0.0899 \cdot p((1,2))}{p((1,2))}}{p(A)}
\]
Since \( p((1,2)) \) cancels out in the numerator and denominator (in 1. a) it also was considered in the denominator without problem of being zero), we simplify to:
\[
p((1,2) | A) = \frac{0.0899}{p(A)} = \frac{0.0899}{0.5} = 0.1798
\]

And finally substitute to calculate \( p(A, x_\text{query}) \):
\[
p(A, x_\text{query}) = p((1,2) | A) \cdot p(1 | A) \cdot p(A) = 
\]
\[
= 0.1798 \cdot 0.5 \cdot 0.5 = 0.04495
\]

\bigskip

Now, letâ€™s calculate the same for class \( B \). Using the same steps:

From the previous calculation:
\[
p(B | (1,2)) = \frac{0.0019}{p((1,2))}
\]
Rearranging for \( p((1,2) | B) \):
\[
p((1,2) | B) = \frac{p(B | (1,2)) \cdot p((1,2))}{p(B)}
\]
Substituting known values:
\[
p((1,2) | B) = \frac{0.0019}{0.5} = 0.0038
\]

Finally, we calculate \( p(B, x_\text{query}) \):
\[
p(B, x_\text{query}) = p((1,2) | B) \cdot p(1 | B) \cdot p(B) =
\]
\[
= 0.0038 \cdot 0.75 \cdot 0.5 = 0.001425
\]

\bigskip

We now have both probabilities:
\[
p(A, x_\text{query}) = 0.04495
\]
\[
p(B, x_\text{query}) = 0.001425
\]

Since \( p(A, x_\text{query}) > p(B, x_\text{query}) \), the most probable class for the query vector \( x_\text{query} = (1,2) \) is class \( A \).



\subsection{Exercise dd) Martim}
In this exercise, the target is similar to the previous one: to find the most probable class, but now we have to indicate the relative probability between classes as well.
\newline
\newline
Giving the probabilities:

\[
P(A) = \frac{1}{2},\;P(B) = \frac{1}{2}
\]

We can calculate the probability of \( x_3 \) being 1, with the class being both A and B:

\[
P(x_3 = 1 \mid A) = \frac{card(A.1)}{card(A)} = \frac{1}{2}
\]
\[
P(x_3 = 1 \mid B) = \frac{card(B.1)}{card(B)} = \frac{3}{4}
\]
\newline
Now, we have to calculate the probability that the entry (1,2) belongs to both A and B

Since we already have \( P(A) \) and \( P(B) \), we only need to calculate both \( P((1,2) \mid A) \) and \( P((1,2) \mid B) \)
we need firstly to estimate \( \mu \) (average) and \(\sigma^2\) (variance) for both A and B.

\[
\mu = \frac{1}{n}\sum_{i = 1}^n x_i, \;\;\sigma^2 = \frac{1}{n}\sum_{i = 1}^n (x_i - \mu)^2
\]

\[
\mu_A(x_1) = \frac{0.6+1+1.6+1.8}{4} = 1.25
\]
\[
\mu_A(x_2) = \frac{0.4+1.1+0.5+1.8}{4} = 0.95
\]
\[
\mu_B(x_1) = \frac{2+2+3+4}{4} = 2.75
\]
\[
\mu_B(x_2) = \frac{0+1+0+1.2}{4} = 0.55
\]

\[
\sigma^2_A(x_1) = \frac{(0.6-1.25)^2 + (1 - 1.25)^2 + (1.6 - 1.25)^2 + (1.8 - 1.25)^2}{4} = 0.2275
\]
\[
\sigma^2_A(x_2) = \frac{(0.6-0.95)^2 + (1 - 0.95)^2 + (1.6 - 0.95)^2 + (1.8 - 0.95)^2}{4} = 0.3175
\]
\[
\sigma^2_B(x_1) = \frac{(2 - 2.75)^2 + (2 - 2.75)^2 + (3 - 2.75)^2 + (4 - 2.75)^2}{4} = 0.6875
\]
\[
\sigma^2_B(x_2) = \frac{(0 - 0.55)^2 + (1 - 0.55)^2 + (0 - 0.55)^2 + (1.2 - 0.55)^2}{4} = 0.3075
\]














\newpage
% Apontamentos para a formula original
% N \left( \mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma} \right) = \frac{1}{\left( 2 \pi \right)^{\frac{2}{2}} \sqrt{0.8333}} \exp \left( -\frac{1}{2} \left( \begin{bmatrix} x_0 \\ x_1 \end{bmatrix} - \begin{bmatrix} -1.25 \\ 1.75 \end{bmatrix} \right)^\top \begin{bmatrix} 1.1 & 0.1 \\ 0.1 & 1.1 \end{bmatrix} \left( \begin{bmatrix} x_0 \\ x_1 \end{bmatrix} - \begin{bmatrix} -1.25 \\ 1.75 \end{bmatrix} \right) \right)





\section{Software Experiment} 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{}          & \textbf{NIST test} & \textbf{Wine test} \\ \hline
\textbf{Train/Test Size} & 718 / 1079 & 71 / 107                \\ \hline
\textbf{kNN (k = 3)}     & 0.98                     & 0.69                     \\ \hline
\textbf{kNN (k = 30)}    & 0.95                     & 0.67                     \\ \hline
\textbf{Bayesian (Gauss)}& 0.87                     & 0.98                     \\ \hline
\end{tabular}
\caption{Combined results from NIST and Wine experiments.}
\end{table}

The kNN method gives better results (greater accuracy) for the NMIST dataset, while the Gaussian Naive Baye's approach (GaussianNB) was better for the wine dataset, and this is caused by two main aspects: the Curse of Dimensionality and the type of distribution.
\\
\\
From one side, kNN performs surprisingly well with the NMIST (digits) dataset, and just a bit worse when the high number of neighbours (K = 30) leads to similar features (a similar input value) being interpreted as the same. Noteworthy, in this case the Curse of Dimensionality did not influence kNN because all attributes (each of the 64 matrix input' cells) are equally important. GaussianNB, however, performs worse on the dataset, because it relates less to a Gaussian distribution. 
\\
\\
From the other side, wine's data distribution can be well-approximated by a Gaussian distribution considering the Naive Baye's approach, leading to more accurate results. Meanwhile, kNN's lazy learning here does diminish the accuracy (specially for K = 30, for the same reason as above). Here, kNN's worse accuracy is explained by the due to the instance space being high-dimensional and also not equally relevant, consequently misleading the importance of some features (Curse of dimensionality). For instance, the color of the wine is more important than the amount of alcohol, for example.







%---------------------------------------------------------------------------------------
\newpage

\end{document}
