\documentclass{article}
\usepackage{amsmath}  % For math symbols
\usepackage{array}

\title{Machine Learning Report}
\author{}
\date{}
\begin{document}

\maketitle
\begin{center}
    \textbf{Homework 2 - Bayesian Learning}
\end{center}

\section{Bayesian Classifier}
\subsection{Exercise a)}
We will first estimate the priors.

$$p(Class = A) = \frac{4}{8} = \frac{1}{2}$$
$$p(Class = B) = \frac{4}{8} = \frac{1}{2}$$

Now, to estimate the likelihoods, we must consider each distribution as independent (Naive Bayes's assumption), so it will be:
\\
\begin{tabular}{|c|c|c|}
    \hline
    & \( p(x_1 \mid \text{Class} = A) \) & \( p(x_1 \mid \text{Class} = B) \) \\ 
    \hline
    \( \mu \) & 20 & 20 \\
    \hline
    \( \sigma \) & 20 & 20 \\
    \hline
\end{tabular}
\\
\\
Now, for the query vector:
\begin{align*}
    p(\text{Class} = A \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1 \mid \text{Class} = A) \cdot p(x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(1 \mid \mu = 1.25, \sigma = 0.5508) \cdot N(2 \mid \mu = 1.2, \sigma = 0.6055)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{0.0899}{p(x_1 = 1, x_2 = 2)}
\end{align*}


\begin{align*}
    p(\text{Class} = B \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1 \mid \text{Class} = B) \cdot p(x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(1 \mid \mu = 2.7500\sigma = 0.9574) \cdot N(2 \mid \mu = 0.5500, \sigma = 0.6403)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{0.0019}{p(x_1 = 1, x_2 = 2)}
\end{align*}
\\
By comparing the numerators, we conclude that:
$$p(\text{Class} = A \mid x_1 = 1, x_2 = 2) > p(\text{Class} = B \mid x_1 = 1, x_2 = 2)$$
\\
Hence, the most probable class for the query vector $$x = (1, 2)\textsuperscript{T}$$ is class A.
\newpage



\subsection{Exercise b)}
As before, the priors have the following probabilities:

$$p(Class = A) = \frac{1}{2}$$
$$p(Class = B) = \frac{1}{2}$$

Now, to find the parameters of the two class conditional 2-d Gaussians that model the likelihoods:
\\
\\
\begin{tabular}{|c|c|c|}
    \hline
    & \( p(x_1, x_2 \mid \text{Class} = A) \) & \( p(x_1, x_2 \mid \text{Class} = B) \) \\ 
    \hline
    \( \mu \) & $\begin{pmatrix} 1.25 \\ 1.20 \end{pmatrix}$ & $\begin{pmatrix} 2.75 \\  0.55 \end{pmatrix}$ \\
    \hline
    \( \sum \) & $\begin{pmatrix} 0.3033 & 0.3267 \\ 0.3267 & 0.36667 \end{pmatrix}$ & $\begin{pmatrix} 0.9166 & 0.2500 \\ 0.2500 & 0.4100 \end{pmatrix}$ \\
    \hline
\end{tabular}
\\
\\
Now, calculating the posteriors:
\begin{align*}
    p(\text{Class} = A \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = A) \cdot p(x_1 = 1 \mid \text{Class} = A) \cdot p(x_2 = 2 \mid \text{Class} = A)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(
        \begin{pmatrix}1 \\ 2\end{pmatrix} \mid
        \mu = \begin{pmatrix}1.25 \\ 1.20\end{pmatrix},
        \sum = \begin{pmatrix}0.3033 & 0.3267 \\ 0.3267 & 0.36667 \end{pmatrix}
    )}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{4.3346 \times 10\textsuperscript{-17}}{p(x_1 = 1, x_2 = 2)}
\end{align*}

\begin{align*}
    p(\text{Class} = B \mid x_1 = 1, x_2 = 2) 
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1, x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{p(\text{Class} = B) \cdot p(x_1 = 1 \mid \text{Class} = B) \cdot p(x_2 = 2 \mid \text{Class} = B)}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{\frac{1}{2} \cdot N(
        \begin{pmatrix}1 \\ 2\end{pmatrix} \mid
        \mu = \begin{pmatrix}2.75 \\ 0.55\end{pmatrix},
        \sum = \begin{pmatrix}0.9167 & 0.2500 \\ 0.2500 & 0.4100 \end{pmatrix}
    )}{p(x_1 = 1, x_2 = 2)} \\
    &= \frac{2.3373\times10\textsuperscript{-4}}{p(x_1 = 1, x_2 = 2)}
\end{align*}

By comparing the numerators, we conclude that:
$$p(\text{Class} = A \mid x_1 = 1, x_2 = 2) < p(\text{Class} = B \mid x_1 = 1, x_2 = 2)$$
\\
Hence, the most probable class for the query vector $$x = (1, 2)\textsuperscript{T}$$ is class B.

The predicted class is not the same, what may indicate that the parameters of x1 and x2 are not independent, making a Naive Gaussian distribution inadequate for this situation.


"the parameters x1x1​ and x2x2​ are likely not independent"
"a more complex model that accounts for the joint distribution of the features could provide better predictive accuracy"
\newpage


\subsection{Exercise c)}
In this exercise, we want to determine which class (A or B) appears the most times when \( x_3 = 1 \).

To compute that, we use the Bayes' rule of interpretation:

\[
P(C \mid E) = \frac{P(E \mid C) \cdot P(C)}{P(E)}
\]

Where:
\begin{itemize}
    \item \( P(C \mid E) \) is the posterior probability of the class \( C \), given the evidence \( E \) (in this case, \( x_3 = 1 \)).
    \item \( P(E \mid C) \) is the likelihood of observing \( x_3 = 1 \) given class \( C \).
    \item \( P(C) \) is the prior probability of class \( C \) (the general probability of class \( C \)).
    \item \( P(E) \) is the total probability of observing \( x_3 = 1 \) (the normalizing factor).
\end{itemize}

Given the following probabilities:
\[
P(A) = \frac{1}{2},\;P(B) = \frac{1}{2},\;P(E \mid A ) = \frac{1}{2},\;P(E \mid B) = \frac{3}{4},\;P(E) = \frac{5}{8}
\]

We can now calculate both final probabilities:

\[P(A \mid E) = \frac{\frac{1}{2} \times \frac{1}{2}}{\frac{5}{8}} = \frac{2}{5}\]

\[P(B \mid E) = \frac{\frac{1}{2} \times \frac{3}{4}}{\frac{5}{8}} = \frac{3}{5}\]

So, with the calculations, we conclude that the most probable class is B.

\newpage

\subsection{Exercise d)}
In this exercise, the target is similar to the previous one: to find the most probable class, but now we have to indicate the relative probability between classes as well.
\newline
\newline
Giving the probabilities:

\[
P(A) = \frac{1}{2},\;P(B) = \frac{1}{2}
\]

We can calculate the probability of \( x_3 \) being 1, with the class being both A and B:

\[
P(x_3 = 1 \mid A) = \frac{card(A.1)}{card(A)} = \frac{1}{2}
\]
\[
P(x_3 = 1 \mid B) = \frac{card(B.1)}{card(B)} = \frac{3}{4}
\]
\newline
Now, we have to calculate the probability that the entry (1,2) belongs to both A and B

Since we already have \( P(A) \) and \( P(B) \), we only need to calculate both \( P((1,2) \mid A) \) and \( P((1,2) \mid B) \)
we need firstly to estimate \( \mu \) (average) and \(\sigma^2\) (variance) for both A and B.

\[
\mu = \frac{1}{n}\sum_{i = 1}^n x_i, \;\;\sigma^2 = \frac{1}{n}\sum_{i = 1}^n (x_i - \mu)^2
\]

\[
\mu_A(x_1) = \frac{0.6+1+1.6+1.8}{4} = 1.25
\]
\[
\mu_A(x_2) = \frac{0.4+1.1+0.5+1.8}{4} = 0.95
\]
\[
\mu_B(x_1) = \frac{2+2+3+4}{4} = 2.75
\]
\[
\mu_B(x_2) = \frac{0+1+0+1.2}{4} = 0.55
\]

\[
\sigma^2_A(x_1) = \frac{(0.6-1.25)^2 + (1 - 1.25)^2 + (1.6 - 1.25)^2 + (1.8 - 1.25)^2}{4} = 0.2275
\]
\[
\sigma^2_A(x_2) = \frac{(0.6-0.95)^2 + (1 - 0.95)^2 + (1.6 - 0.95)^2 + (1.8 - 0.95)^2}{4} = 0.3175
\]
\[
\sigma^2_B(x_1) = \frac{(2 - 2.75)^2 + (2 - 2.75)^2 + (3 - 2.75)^2 + (4 - 2.75)^2}{4} = 0.6875
\]
\[
\sigma^2_B(x_2) = \frac{(0 - 0.55)^2 + (1 - 0.55)^2 + (0 - 0.55)^2 + (1.2 - 0.55)^2}{4} = 0.3075
\]
%---------------------------------------------------------------------------------------
\newpage

\end{document}
